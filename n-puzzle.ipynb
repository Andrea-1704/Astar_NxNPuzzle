{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deadline: 24 novembre ore 23:59.\n",
    "Solve efficiently a generic n^2-1 puzzle using path search algorithm.\n",
    "\n",
    "Cost=  total number of actions you need to __evaluate__. An action is something that bring me to a new state. For example the number of swaps to do.\n",
    "\n",
    "The result is the sequence of action that took you at the end. The goal is not to find a state but a sequence of actions from srtarting point to end point: we do not look for a soluzion but for a sequence of actions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import namedtuple\n",
    "from random import choice\n",
    "from tqdm.auto import tqdm\n",
    "import numpy as np\n",
    "from heapq import heappop, heappush"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "PUZZLE_DIM = 4\n",
    "action = namedtuple('Action', ['pos1', 'pos2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def available_actions(state: np.ndarray) -> list['Action']:\n",
    "    x, y = [int(_[0]) for _ in np.where(state == 0)]\n",
    "    actions = list()\n",
    "    if x > 0:\n",
    "        actions.append(action((x, y), (x - 1, y)))\n",
    "    if x < PUZZLE_DIM - 1:\n",
    "        actions.append(action((x, y), (x + 1, y)))\n",
    "    if y > 0:\n",
    "        actions.append(action((x, y), (x, y - 1)))\n",
    "    if y < PUZZLE_DIM - 1:\n",
    "        actions.append(action((x, y), (x, y + 1)))\n",
    "    return actions\n",
    "\n",
    "\n",
    "\n",
    "def do_action(state: np.ndarray, action: 'Action') -> np.ndarray:\n",
    "    new_state = state.copy()\n",
    "    new_state[action.pos1], new_state[action.pos2] = new_state[action.pos2], new_state[action.pos1]\n",
    "    return new_state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The state is a numpy array.\n",
    "\n",
    "We created a function that returns the number of actions from a state pos1 to a state pos2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute 100_000 random actions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Randomizing: 100%|██████████| 100000/100000 [00:01<00:00, 60669.25it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 6,  8, 13,  2],\n",
       "       [10,  9,  3,  7],\n",
       "       [11,  4,  0,  1],\n",
       "       [14, 12, 15,  5]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RANDOMIZE_STEPS = 100_000\n",
    "state = np.array([i for i in range(1, PUZZLE_DIM**2)] + [0]).reshape((PUZZLE_DIM, PUZZLE_DIM))\n",
    "for r in tqdm(range(RANDOMIZE_STEPS), desc='Randomizing'):\n",
    "    state = do_action(state, choice(available_actions(state)))\n",
    "state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a function that indicates if we end the search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_goal(solution):\n",
    "    arr_solution = np.reshape(solution, PUZZLE_DIM*PUZZLE_DIM)\n",
    "    arr_solution_no_zero = arr_solution[0: len(arr_solution)-1]\n",
    "    if np.all(arr_solution_no_zero[:-1] <= arr_solution_no_zero[1:]) and arr_solution[len(arr_solution)-1]==0:\n",
    "        return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Depth search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use a function that computes a value for each matrix so that it will be easy to store matrices.\n",
    "\n",
    "This function shouuld be bi-directional: a number is associated to only one matrix and viceversa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def matrix_score(matrix):\n",
    "    \"\"\"\n",
    "    Performs the product for each element with the position in the matrix.\n",
    "    \n",
    "    Parameters:\n",
    "    - state (np.ndarray): A state of the puzzle.\n",
    "    \n",
    "    Returns:\n",
    "    - Score of the matrix\n",
    "    \"\"\"\n",
    "    #flatter the matrix:\n",
    "    matrice = matrix.flatten()\n",
    "    score = 0\n",
    "    for i in range(len(matrice)):\n",
    "        score += (10**i)*matrice[i]\n",
    "    return int(score)\n",
    "\n",
    "\n",
    "def simplified_matrix_score(matrix):\n",
    "    \"\"\"\n",
    "    Performs the product for each element with the position in the matrix.\n",
    "    \n",
    "    Parameters:\n",
    "    - state (np.ndarray): A state of the puzzle.\n",
    "    \n",
    "    Returns:\n",
    "    - Score of the matrix\n",
    "    \"\"\"\n",
    "    #flatter the matrix:\n",
    "    matrice = matrix.flatten()\n",
    "    score = 0\n",
    "    for i in range(len(matrice)):\n",
    "        score += i*matrice[i]\n",
    "    return int(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import namedtuple, deque\n",
    "import numpy as np\n",
    "\n",
    "# Define action as a named tuple\n",
    "action = namedtuple('Action', ['pos1', 'pos2'])\n",
    "\n",
    "def depth_limited_search(initial_state: np.ndarray, final_state: np.ndarray, max_depth: int) -> list[action] or None:\n",
    "    \"\"\"\n",
    "    Performs a depth-limited search to solve the n-puzzle.\n",
    "    \n",
    "    Parameters:\n",
    "    - initial_state (np.ndarray): The starting state of the puzzle.\n",
    "    - final_state (np.ndarray): The desired goal state of the puzzle.\n",
    "    - max_depth (int): The maximum depth limit for the search.\n",
    "    \n",
    "    Returns:\n",
    "    - list[action] or None: A sequence of actions leading to the solution, or None if no solution is found.\n",
    "    \"\"\"\n",
    "    stack = deque([(initial_state, [], 0)])  # Stack of (current state, path to reach it, current depth)\n",
    "    visited = set()  # Set of visited states for current path only\n",
    "    optimum = matrix_score(final_state)\n",
    "\n",
    "    while stack:\n",
    "        current_state, path, depth = stack.pop()\n",
    "        current_score = matrix_score(current_state)\n",
    "\n",
    "        # Check if we reached the goal\n",
    "        if current_score == optimum:\n",
    "            return path\n",
    "        \n",
    "        # # Backtrack if depth limit reached\n",
    "        # if depth >= max_depth:\n",
    "        #     continue\n",
    "        \n",
    "        # Add current state to visited set (track only in current path)\n",
    "        visited.add(current_score)\n",
    "        \n",
    "        # Generate and iterate over all possible moves\n",
    "        for act in available_actions(current_state):\n",
    "            next_state = do_action(current_state, act)\n",
    "            \n",
    "            # Check if the next state has already been visited in the current path\n",
    "            if matrix_score(next_state) not in visited:\n",
    "                # Add the new state and path to stack, increase depth\n",
    "                stack.append((next_state, path + [act], depth + 1))\n",
    "        \n",
    "        # Remove the current state from visited set after backtracking\n",
    "        #visited.remove(matrix_score(current_state))\n",
    "    \n",
    "    return None  # Return None if no solution is found within depth limit\n",
    "\n",
    "# Iterative Deepening Depth-First Search (IDDFS) wrapper function\n",
    "def iterative_deepening_dfs(initial_state: np.ndarray, final_state: np.ndarray, max_depth: int = 50) -> list[action] or None:\n",
    "    \"\"\"\n",
    "    Performs an iterative deepening DFS to solve the n-puzzle.\n",
    "    \n",
    "    Parameters:\n",
    "    - initial_state (np.ndarray): The starting state of the puzzle.\n",
    "    - final_state (np.ndarray): The desired goal state of the puzzle.\n",
    "    - max_depth (int): The maximum depth to limit the search for IDDFS.\n",
    "    \n",
    "    Returns:\n",
    "    - list[action] or None: A sequence of actions leading to the solution, or None if no solution is found.\n",
    "    \"\"\"\n",
    "    for depth in range(1, max_depth + 1):\n",
    "        result = depth_limited_search(initial_state, final_state, depth)\n",
    "        if result is not None:\n",
    "            return result\n",
    "    return None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Updated Depth search\n",
    "\n",
    "At each iteration, among all the possible we choose the one with the minimum manahattan distance to the test goal.\n",
    "In case there is no state with less manhattan distance compared to the current we select the oen that is the \"best\" among the possibilities. In case they are the same we use the last one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_depth(initial_state: np.ndarray, final_state: np.ndarray) -> list or None:\n",
    "    stack = deque([(initial_state, [], 0)])  # Stack of (current state, path to reach it, current depth)\n",
    "    visited = set()  # Set of visited states for current path only\n",
    "    optimum = matrix_score(final_state)\n",
    "\n",
    "    while stack:\n",
    "        current_state, path, depth = stack.pop()\n",
    "        current_score = matrix_score(current_state)\n",
    "\n",
    "        # Check if we reached the goal\n",
    "        if current_score == optimum:\n",
    "            return path\n",
    "        \n",
    "        # Add current state to visited set (track only in current path)\n",
    "        visited.add(current_score)\n",
    "        \n",
    "        # Generate and calculate Manhattan distances for all possible moves\n",
    "        successors = []\n",
    "        for act in available_actions(current_state):\n",
    "            next_state = do_action(current_state, act)\n",
    "            next_score = matrix_score(next_state)\n",
    "            \n",
    "            # Check if the next state has already been visited in the current path\n",
    "            if next_score not in visited:\n",
    "                # Calculate the Manhattan distance from the goal\n",
    "                manhattan_dist = manhattan_distance(next_state, final_state)\n",
    "                # Append (next_state, path + [act], depth + 1, manhattan_dist) to successors list\n",
    "                successors.append((next_state, path + [act], depth + 1, manhattan_dist))\n",
    "        \n",
    "        # Sort successors by Manhattan distance (ascending)\n",
    "        successors.sort(key=lambda x: x[3])  # Sort by the fourth item, which is manhattan_dist\n",
    "        \n",
    "        # Add sorted successors to the stack\n",
    "        for next_state, new_path, new_depth, _ in successors:\n",
    "            stack.append((next_state, new_path, new_depth))\n",
    "        \n",
    "        # Remove the current state from visited set after backtracking\n",
    "        #visited.remove(current_score)  # Uncomment if backtracking is implemented\n",
    "    \n",
    "    return None  # Return None if no solution is found within depth limit\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A*\n",
    "In the context of the n-puzzle problem, **Manhattan distance** is a heuristic used to estimate how close a puzzle state is to the goal. For two matrices (representing the current state and the goal state), Manhattan distance is calculated by summing up the individual distances of each tile from its target position.\n",
    "\n",
    "1. **Tile Distance**: For each tile, the distance is the absolute difference between its current row and target row, plus the absolute difference between its current column and target column.\n",
    "   \n",
    "2. **Total Distance**: Summing these values for all tiles gives the Manhattan distance for the puzzle. This value represents the minimum number of moves required to reach the goal state if we could move each tile independently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def manhattan_distance(state: np.ndarray, goal_state: np.ndarray) -> int:\n",
    "    \"\"\"\n",
    "    Calculates the Manhattan distance for each tile in the puzzle.\n",
    "    \"\"\"\n",
    "    distance = 0\n",
    "    for value in range(1, state.size):  # Skip 0 (empty space)\n",
    "        target_pos = np.argwhere(goal_state == value)[0]\n",
    "        current_pos = np.argwhere(state == value)[0]\n",
    "        distance += abs(target_pos[0] - current_pos[0]) + abs(target_pos[1] - current_pos[1])\n",
    "    return distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def a_star(initial_state: np.ndarray, final_state: np.ndarray) -> list[action] or None:\n",
    "    \"\"\"\n",
    "    Performs an A* search to solve the n-puzzle.\n",
    "    \n",
    "    Parameters:\n",
    "    - initial_state (np.ndarray): The starting state of the puzzle.\n",
    "    - final_state (np.ndarray): The desired goal state of the puzzle.\n",
    "    \n",
    "    Returns:\n",
    "    - list[action] or None: A sequence of actions leading to the solution, or None if no solution is found.\n",
    "    \"\"\"\n",
    "    # Priority queue: (f_score, g_score, current_state, path)\n",
    "    open_set = []\n",
    "    heappush(open_set, (manhattan_distance(initial_state, final_state), 0, initial_state.tobytes(), []))\n",
    "    #this function push into open_set a new element continuing to garantee the heap properties.\n",
    "    # the element that we insert are 4.\n",
    "    \n",
    "    visited = set()  \n",
    "    \n",
    "    while open_set:\n",
    "        f_score, g_score, current_bytes, path = heappop(open_set)\n",
    "        current_state = np.frombuffer(current_bytes, dtype=initial_state.dtype).reshape(initial_state.shape)\n",
    "        \n",
    "        # Check if we reached the goal\n",
    "        if np.array_equal(current_state, final_state):\n",
    "            return path\n",
    "        \n",
    "        # Mark current state as visited\n",
    "        visited.add(current_bytes)\n",
    "        \n",
    "        # Generate and iterate over all possible moves\n",
    "        for act in available_actions(current_state):\n",
    "            next_state = do_action(current_state, act)\n",
    "            next_bytes = next_state.tobytes()\n",
    "            if next_bytes in visited:\n",
    "                continue\n",
    "            \n",
    "            # Calculate scores for the next state\n",
    "            new_g_score = g_score + 1\n",
    "            new_f_score = new_g_score + manhattan_distance(next_state, final_state)\n",
    "            \n",
    "            # Push the new state into the priority queue with updated scores\n",
    "            heappush(open_set, (new_f_score, new_g_score, next_bytes, path + [act]))\n",
    "    \n",
    "    return None  # Return None if no solution is found"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this code we are storing all the state and comparing them. This may be difficoult. Let's try to use the function that computes a singol number for a given state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def a_star_no_storing(initial_state: np.ndarray, final_state: np.ndarray) -> list[action] or None:\n",
    "    \"\"\"\n",
    "    Performs an A* search to solve the n-puzzle.\n",
    "    \n",
    "    Parameters:\n",
    "    - initial_state (np.ndarray): The starting state of the puzzle.\n",
    "    - final_state (np.ndarray): The desired goal state of the puzzle.\n",
    "    \n",
    "    Returns:\n",
    "    - list[action] or None: A sequence of actions leading to the solution, or None if no solution is found.\n",
    "    \"\"\"\n",
    "    # Priority queue: (f_score, g_score, current_state, path)\n",
    "    open_set = []\n",
    "    heappush(open_set, (manhattan_distance(initial_state, final_state), 0, initial_state.tobytes(), []))\n",
    "    #this function push into open_set a new element continuing to garantee the heap properties.\n",
    "    # the element that we insert are 4.\n",
    "    \n",
    "    visited = set()  \n",
    "    optimum = matrix_score(final_state)\n",
    "    \n",
    "    while open_set:\n",
    "        f_score, g_score, current_bytes, path = heappop(open_set)\n",
    "        current_state = np.frombuffer(current_bytes, dtype=initial_state.dtype).reshape(initial_state.shape)\n",
    "        current_score = matrix_score(current_state)\n",
    "\n",
    "        # Check if we reached the goal\n",
    "        if current_score == optimum:\n",
    "            return path\n",
    "        \n",
    "        # Mark current state as visited\n",
    "        visited.add(current_score)\n",
    "        #visited.add(current_bytes)\n",
    "        \n",
    "        # Generate and iterate over all possible moves\n",
    "        for act in available_actions(current_state):\n",
    "            next_state = do_action(current_state, act)\n",
    "            next_bytes = next_state.tobytes()\n",
    "            value = matrix_score(next_state)\n",
    "            if value in visited:\n",
    "                continue\n",
    "            \n",
    "            # Calculate scores for the next state\n",
    "            new_g_score = g_score + 1\n",
    "            new_f_score = new_g_score + manhattan_distance(next_state, final_state)\n",
    "            \n",
    "            # Push the new state into the priority queue with updated scores\n",
    "            heappush(open_set, (new_f_score, new_g_score, next_bytes, path + [act]))\n",
    "    \n",
    "    return None  # Return None if no solution is found"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The problem is that still we are not able to solve 4*4 puzzle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def state_to_tuple(state: np.ndarray) -> tuple:\n",
    "    return tuple(state.flatten())\n",
    "\n",
    "def linear_conflict(state, goal):\n",
    "    conflict = 0\n",
    "    size = state.shape[0]\n",
    "    for row in range(size):\n",
    "        row_goal = goal[row]\n",
    "        row_state = state[row]\n",
    "        for i in range(size):\n",
    "            for j in range(i + 1, size):\n",
    "                if row_state[i] in row_goal and row_state[j] in row_goal and row_state[i] > row_state[j]:\n",
    "                    conflict += 2\n",
    "    return conflict\n",
    "\n",
    "def enhanced_heuristic(state, goal):\n",
    "    return manhattan_distance(state, goal) + linear_conflict(state, goal)\n",
    "\n",
    "\n",
    "def enhanced_a_star(initial_state: np.ndarray, final_state: np.ndarray) -> list[action] or None:\n",
    "    # Priority queue: (f_score, g_score, current_state, path)\n",
    "    open_set = []\n",
    "    heappush(open_set, (enhanced_heuristic(initial_state, final_state), 0, initial_state.tobytes(), []))\n",
    "    visited = set()  \n",
    "    optimum = state_to_tuple(final_state)\n",
    "    \n",
    "    while open_set:\n",
    "        f_score, g_score, current_bytes, path = heappop(open_set)\n",
    "        current_state = np.frombuffer(current_bytes, dtype=initial_state.dtype).reshape(initial_state.shape)\n",
    "        current_score = state_to_tuple(current_state)\n",
    "\n",
    "        # Check if we reached the goal\n",
    "        if current_score == optimum:\n",
    "            return path\n",
    "        \n",
    "        # Mark current state as visited\n",
    "        visited.add(current_score)\n",
    "        #visited.add(current_bytes)\n",
    "        \n",
    "        # Generate and iterate over all possible moves\n",
    "        for act in available_actions(current_state):\n",
    "            next_state = do_action(current_state, act)\n",
    "            next_bytes = next_state.tobytes()\n",
    "            value = state_to_tuple(next_state)\n",
    "            if value in visited:\n",
    "                continue\n",
    "            \n",
    "            # Calculate scores for the next state\n",
    "            new_g_score = g_score + 1\n",
    "            new_f_score = new_g_score + enhanced_heuristic(next_state, final_state)\n",
    "            \n",
    "            # Push the new state into the priority queue with updated scores\n",
    "            heappush(open_set, (new_f_score, new_g_score, next_bytes, path + [act]))\n",
    "    \n",
    "    return None  # Return None if no solution is found"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PuzzleHeuristicService:\n",
    "    def __init__(self, goal_state: np.ndarray):\n",
    "        self.goal_state = goal_state\n",
    "\n",
    "    def manhattan_distance(self, state: np.ndarray) -> int:\n",
    "        size = state.shape[0]\n",
    "        dist = 0\n",
    "        for i in range(size):\n",
    "            for j in range(size):\n",
    "                value = state[i, j]\n",
    "                if value != 0:  # Ignora il tassello vuoto\n",
    "                    target_x, target_y = divmod(np.where(self.goal_state.flatten() == value)[0][0], size)\n",
    "                    dist += abs(i - target_x) + abs(j - target_y)\n",
    "        return dist\n",
    "\n",
    "    def linear_conflict(self, state: np.ndarray) -> int:\n",
    "        conflict = 0\n",
    "        size = state.shape[0]\n",
    "        for row in range(size):\n",
    "            row_goal = self.goal_state[row]\n",
    "            row_state = state[row]\n",
    "            for i in range(size):\n",
    "                for j in range(i + 1, size):\n",
    "                    if (\n",
    "                        row_state[i] in row_goal\n",
    "                        and row_state[j] in row_goal\n",
    "                        and row_state[i] > row_state[j]\n",
    "                    ):\n",
    "                        conflict += 2\n",
    "        return conflict\n",
    "\n",
    "    def combined_heuristic(self, state: np.ndarray) -> int:\n",
    "        return self.manhattan_distance(state) + self.linear_conflict(state)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def enhanced_a_star(initial_state: np.ndarray, final_state: np.ndarray) -> list or None:\n",
    "    \"\"\"\n",
    "    Enhanced A* algorithm for the n-puzzle problem using modular heuristics.\n",
    "    \"\"\"\n",
    "    # Classe per l'euristica\n",
    "    heuristic_service = PuzzleHeuristicService(final_state)\n",
    "\n",
    "    def calculate_heuristic(state: np.ndarray) -> int:\n",
    "        # Usa l'euristica combinata (Manhattan + Linear Conflict)\n",
    "        return heuristic_service.combined_heuristic(state)\n",
    "\n",
    "    # Priority queue: (f_score, g_score, current_state, path)\n",
    "    open_set = []\n",
    "    heappush(open_set, (calculate_heuristic(initial_state), 0, initial_state.tobytes(), []))\n",
    "    visited = set()\n",
    "    optimum = state_to_tuple(final_state)\n",
    "\n",
    "    while open_set:\n",
    "        # Estrai il nodo con il punteggio più basso\n",
    "        f_score, g_score, current_bytes, path = heappop(open_set)\n",
    "        current_state = np.frombuffer(current_bytes, dtype=initial_state.dtype).reshape(initial_state.shape)\n",
    "        current_score = state_to_tuple(current_state)\n",
    "\n",
    "        # Controlla se il nodo corrente è il goal\n",
    "        if current_score == optimum:\n",
    "            return path  # Soluzione trovata\n",
    "\n",
    "        # Aggiungi lo stato corrente ai visitati\n",
    "        visited.add(current_score)\n",
    "\n",
    "        # Genera e valuta le mosse possibili\n",
    "        for act in available_actions(current_state):\n",
    "            next_state = do_action(current_state, act)\n",
    "            next_score = state_to_tuple(next_state)\n",
    "            if next_score in visited:\n",
    "                continue\n",
    "\n",
    "            # Calcola i punteggi per lo stato successivo\n",
    "            new_g_score = g_score + 1\n",
    "            new_f_score = new_g_score + calculate_heuristic(next_state)\n",
    "\n",
    "            # Aggiungi il nuovo stato alla coda di priorità\n",
    "            heappush(open_set, (new_f_score, new_g_score, next_state.tobytes(), path + [act]))\n",
    "\n",
    "    return None  # Nessuna soluzione trovata\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from heapq import heappop, heappush\n",
    "\n",
    "# Utility Functions\n",
    "def state_to_tuple(state: np.ndarray) -> tuple:\n",
    "    \"\"\"Converte uno stato in una tupla immutabile.\"\"\"\n",
    "    return tuple(state.flatten())\n",
    "\n",
    "def manhattan_distance(state: np.ndarray, goal: np.ndarray) -> int:\n",
    "    \"\"\"Calcola la Manhattan Distance tra lo stato attuale e quello finale.\"\"\"\n",
    "    size = state.shape[0]\n",
    "    distance = 0\n",
    "    for x in range(size):\n",
    "        for y in range(size):\n",
    "            value = state[x, y]\n",
    "            if value != 0:  # Ignora lo spazio vuoto\n",
    "                goal_x, goal_y = divmod(np.where(goal == value)[0][0], size)\n",
    "                distance += abs(goal_x - x) + abs(goal_y - y)\n",
    "    return distance\n",
    "\n",
    "def linear_conflict(state: np.ndarray, goal: np.ndarray) -> int:\n",
    "    \"\"\"Calcola le Linear Conflicts.\"\"\"\n",
    "    conflict = 0\n",
    "    size = state.shape[0]\n",
    "    for row in range(size):\n",
    "        row_goal = goal[row]\n",
    "        row_state = state[row]\n",
    "        for i in range(size):\n",
    "            for j in range(i + 1, size):\n",
    "                if row_state[i] in row_goal and row_state[j] in row_goal and row_state[i] > row_state[j]:\n",
    "                    conflict += 2\n",
    "    return conflict\n",
    "\n",
    "def enhanced_heuristic(state: np.ndarray, goal: np.ndarray) -> int:\n",
    "    \"\"\"Combina Manhattan Distance e Linear Conflict.\"\"\"\n",
    "    return manhattan_distance(state, goal) + linear_conflict(state, goal)\n",
    "\n",
    "# Puzzle Solver\n",
    "class EnhancedAStar:\n",
    "    def __init__(self, initial_state: np.ndarray, final_state: np.ndarray):\n",
    "        self.initial_state = initial_state\n",
    "        self.final_state = final_state\n",
    "        self.visited = set()\n",
    "        self.goal_tuple = state_to_tuple(final_state)\n",
    "\n",
    "    def solve(self):\n",
    "        \"\"\"\n",
    "        Risolve il puzzle utilizzando una versione ottimizzata dell'algoritmo A*.\n",
    "        :return: La sequenza di mosse per arrivare alla soluzione o None se non esiste.\n",
    "        \"\"\"\n",
    "        open_set = []\n",
    "        initial_heuristic = enhanced_heuristic(self.initial_state, self.final_state)\n",
    "        heappush(open_set, (initial_heuristic, 0, self.initial_state.tobytes(), []))\n",
    "\n",
    "        while open_set:\n",
    "            f_score, g_score, current_bytes, path = heappop(open_set)\n",
    "            current_state = np.frombuffer(current_bytes, dtype=self.initial_state.dtype).reshape(self.initial_state.shape)\n",
    "            current_score = state_to_tuple(current_state)\n",
    "\n",
    "            # Controlla se lo stato attuale è quello finale\n",
    "            if current_score == self.goal_tuple:\n",
    "                return path\n",
    "\n",
    "            # Se lo stato è già stato visitato, salta\n",
    "            if current_score in self.visited:\n",
    "                continue\n",
    "            self.visited.add(current_score)\n",
    "\n",
    "            # Genera le mosse disponibili\n",
    "            for action, next_state in self._get_available_actions(current_state):\n",
    "                next_score = state_to_tuple(next_state)\n",
    "                if next_score in self.visited:\n",
    "                    continue\n",
    "\n",
    "                # Calcola i punteggi per il nuovo stato\n",
    "                g_new = g_score + 1\n",
    "                f_new = g_new + enhanced_heuristic(next_state, self.final_state)\n",
    "\n",
    "                # Aggiungi il nuovo stato alla coda di priorità\n",
    "                heappush(open_set, (f_new, g_new, next_state.tobytes(), path + [action]))\n",
    "\n",
    "        return None  # Nessuna soluzione trovata\n",
    "\n",
    "    def _get_available_actions(self, state: np.ndarray):\n",
    "        \"\"\"Genera le mosse disponibili dallo stato corrente.\"\"\"\n",
    "        moves = []\n",
    "        zero_pos = np.argwhere(state == 0)[0]\n",
    "        directions = {\n",
    "            'up': (-1, 0),\n",
    "            'down': (1, 0),\n",
    "            'left': (0, -1),\n",
    "            'right': (0, 1)\n",
    "        }\n",
    "\n",
    "        for action, (dx, dy) in directions.items():\n",
    "            new_pos = zero_pos + np.array([dx, dy])\n",
    "            if 0 <= new_pos[0] < state.shape[0] and 0 <= new_pos[1] < state.shape[1]:\n",
    "                next_state = state.copy()\n",
    "                next_state[zero_pos[0], zero_pos[1]], next_state[new_pos[0], new_pos[1]] = \\\n",
    "                    next_state[new_pos[0], new_pos[1]], next_state[zero_pos[0], zero_pos[1]]\n",
    "                moves.append((action, next_state))\n",
    "\n",
    "        return moves\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_goal = np.arange(1, PUZZLE_DIM*PUZZLE_DIM, 1)\n",
    "test_goal = np.append(test_goal, 0)\n",
    "test_goal = test_goal.reshape((PUZZLE_DIM, PUZZLE_DIM))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Try the upgrade depth search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#update_depth(state, test_goal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Try the A* algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#a_star(state, test_goal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[36], line 13\u001b[0m\n\u001b[0;32m      6\u001b[0m final \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([[\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m4\u001b[39m],\n\u001b[0;32m      7\u001b[0m                   [\u001b[38;5;241m5\u001b[39m, \u001b[38;5;241m6\u001b[39m, \u001b[38;5;241m7\u001b[39m, \u001b[38;5;241m8\u001b[39m],\n\u001b[0;32m      8\u001b[0m                   [\u001b[38;5;241m9\u001b[39m, \u001b[38;5;241m10\u001b[39m, \u001b[38;5;241m11\u001b[39m, \u001b[38;5;241m12\u001b[39m],\n\u001b[0;32m      9\u001b[0m                   [\u001b[38;5;241m13\u001b[39m, \u001b[38;5;241m14\u001b[39m, \u001b[38;5;241m15\u001b[39m, \u001b[38;5;241m0\u001b[39m]])\n\u001b[0;32m     11\u001b[0m solver \u001b[38;5;241m=\u001b[39m EnhancedAStar(initial, final)\n\u001b[1;32m---> 13\u001b[0m solution \u001b[38;5;241m=\u001b[39m \u001b[43msolver\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msolve\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSolution:\u001b[39m\u001b[38;5;124m\"\u001b[39m, solution)\n",
      "Cell \u001b[1;32mIn[32], line 71\u001b[0m, in \u001b[0;36mEnhancedAStar.solve\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     69\u001b[0m \u001b[38;5;66;03m# Genera le mosse disponibili\u001b[39;00m\n\u001b[0;32m     70\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m action, next_state \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_available_actions(current_state):\n\u001b[1;32m---> 71\u001b[0m     next_score \u001b[38;5;241m=\u001b[39m \u001b[43mstate_to_tuple\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnext_state\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m next_score \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvisited:\n\u001b[0;32m     73\u001b[0m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[32], line 7\u001b[0m, in \u001b[0;36mstate_to_tuple\u001b[1;34m(state)\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstate_to_tuple\u001b[39m(state: np\u001b[38;5;241m.\u001b[39mndarray) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mtuple\u001b[39m:\n\u001b[0;32m      6\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Converte uno stato in una tupla immutabile.\"\"\"\u001b[39;00m\n\u001b[1;32m----> 7\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mtuple\u001b[39m(\u001b[43mstate\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflatten\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Stato iniziale e finale di esempio\n",
    "initial = np.array([[5, 1, 2, 3],\n",
    "                    [9, 6, 7, 4],\n",
    "                    [13, 14, 11, 8],\n",
    "                    [0, 10, 15, 12]])\n",
    "final = np.array([[1, 2, 3, 4],\n",
    "                  [5, 6, 7, 8],\n",
    "                  [9, 10, 11, 12],\n",
    "                  [13, 14, 15, 0]])\n",
    "\n",
    "solver = EnhancedAStar(initial, final)\n",
    "\n",
    "solution = solver.solve()\n",
    "\n",
    "print(\"Solution:\", solution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[37], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43menhanced_a_star\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_goal\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[31], line 77\u001b[0m, in \u001b[0;36menhanced_a_star\u001b[1;34m(initial_state, final_state)\u001b[0m\n\u001b[0;32m     75\u001b[0m \u001b[38;5;66;03m# Calcola i punteggi per lo stato successivo\u001b[39;00m\n\u001b[0;32m     76\u001b[0m new_g_score \u001b[38;5;241m=\u001b[39m g_score \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m---> 77\u001b[0m new_f_score \u001b[38;5;241m=\u001b[39m new_g_score \u001b[38;5;241m+\u001b[39m \u001b[43mcalculate_heuristic\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnext_state\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     79\u001b[0m \u001b[38;5;66;03m# Aggiungi il nuovo stato alla coda di priorità\u001b[39;00m\n\u001b[0;32m     80\u001b[0m heappush(open_set, (new_f_score, new_g_score, next_state\u001b[38;5;241m.\u001b[39mtobytes(), path \u001b[38;5;241m+\u001b[39m [act]))\n",
      "Cell \u001b[1;32mIn[31], line 47\u001b[0m, in \u001b[0;36menhanced_a_star.<locals>.calculate_heuristic\u001b[1;34m(state)\u001b[0m\n\u001b[0;32m     45\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcalculate_heuristic\u001b[39m(state: np\u001b[38;5;241m.\u001b[39mndarray) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mint\u001b[39m:\n\u001b[0;32m     46\u001b[0m     \u001b[38;5;66;03m# Usa l'euristica combinata (Manhattan + Linear Conflict)\u001b[39;00m\n\u001b[1;32m---> 47\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mheuristic_service\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcombined_heuristic\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[31], line 33\u001b[0m, in \u001b[0;36mPuzzleHeuristicService.combined_heuristic\u001b[1;34m(self, state)\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcombined_heuristic\u001b[39m(\u001b[38;5;28mself\u001b[39m, state: np\u001b[38;5;241m.\u001b[39mndarray) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mint\u001b[39m:\n\u001b[1;32m---> 33\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmanhattan_distance\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlinear_conflict(state)\n",
      "Cell \u001b[1;32mIn[31], line 12\u001b[0m, in \u001b[0;36mPuzzleHeuristicService.manhattan_distance\u001b[1;34m(self, state)\u001b[0m\n\u001b[0;32m     10\u001b[0m         value \u001b[38;5;241m=\u001b[39m state[i, j]\n\u001b[0;32m     11\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m value \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:  \u001b[38;5;66;03m# Ignora il tassello vuoto\u001b[39;00m\n\u001b[1;32m---> 12\u001b[0m             target_x, target_y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdivmod\u001b[39m(np\u001b[38;5;241m.\u001b[39mwhere(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgoal_state\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflatten\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m==\u001b[39m value)[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m], size)\n\u001b[0;32m     13\u001b[0m             dist \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mabs\u001b[39m(i \u001b[38;5;241m-\u001b[39m target_x) \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mabs\u001b[39m(j \u001b[38;5;241m-\u001b[39m target_y)\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m dist\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#enhanced_a_star(state, test_goal)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
