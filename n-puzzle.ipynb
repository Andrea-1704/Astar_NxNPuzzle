{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deadline: 24 novembre ore 23:59.\n",
    "Solve efficiently a generic n^2-1 puzzle using path search algorithm.\n",
    "\n",
    "Cost=  total number of actions you need to __evaluate__. An action is something that bring me to a new state. For example the number of swaps to do.\n",
    "\n",
    "The result is the sequence of action that took you at the end. The goal is not to find a state but a sequence of actions from srtarting point to end point: we do not look for a soluzion but for a sequence of actions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\andrea\\Documents\\Computational Intelligence\\CI\\CI2024_lab3\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from collections import namedtuple\n",
    "from random import choice\n",
    "from tqdm.auto import tqdm\n",
    "import numpy as np\n",
    "from heapq import heappop, heappush"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "PUZZLE_DIM = 3\n",
    "action = namedtuple('Action', ['pos1', 'pos2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def available_actions(state: np.ndarray) -> list['Action']:\n",
    "    x, y = [int(_[0]) for _ in np.where(state == 0)]\n",
    "    actions = list()\n",
    "    if x > 0:\n",
    "        actions.append(action((x, y), (x - 1, y)))\n",
    "    if x < PUZZLE_DIM - 1:\n",
    "        actions.append(action((x, y), (x + 1, y)))\n",
    "    if y > 0:\n",
    "        actions.append(action((x, y), (x, y - 1)))\n",
    "    if y < PUZZLE_DIM - 1:\n",
    "        actions.append(action((x, y), (x, y + 1)))\n",
    "    return actions\n",
    "\n",
    "\n",
    "\n",
    "def do_action(state: np.ndarray, action: 'Action') -> np.ndarray:\n",
    "    new_state = state.copy()\n",
    "    new_state[action.pos1], new_state[action.pos2] = new_state[action.pos2], new_state[action.pos1]\n",
    "    return new_state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The state is a numpy array.\n",
    "\n",
    "We created a function that returns the number of actions from a state pos1 to a state pos2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute 100_000 random actions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Randomizing:   0%|          | 0/100000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Randomizing: 100%|██████████| 100000/100000 [00:01<00:00, 67986.44it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[4, 7, 3],\n",
       "       [1, 0, 8],\n",
       "       [5, 2, 6]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RANDOMIZE_STEPS = 100_000\n",
    "state = np.array([i for i in range(1, PUZZLE_DIM**2)] + [0]).reshape((PUZZLE_DIM, PUZZLE_DIM))\n",
    "for r in tqdm(range(RANDOMIZE_STEPS), desc='Randomizing'):\n",
    "    state = do_action(state, choice(available_actions(state)))\n",
    "state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a function that indicates if we end the search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_goal(solution):\n",
    "    arr_solution = np.reshape(solution, PUZZLE_DIM*PUZZLE_DIM)\n",
    "    arr_solution_no_zero = arr_solution[0: len(arr_solution)-1]\n",
    "    if np.all(arr_solution_no_zero[:-1] <= arr_solution_no_zero[1:]) and arr_solution[len(arr_solution)-1]==0:\n",
    "        return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Depth search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use a function that computes a value for each matrix so that it will be easy to store matrices.\n",
    "\n",
    "This function shouuld be bi-directional: a number is associated to only one matrix and viceversa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def matrix_score(matrix):\n",
    "    \"\"\"\n",
    "    Performs the product for each element with the position in the matrix.\n",
    "    \n",
    "    Parameters:\n",
    "    - state (np.ndarray): A state of the puzzle.\n",
    "    \n",
    "    Returns:\n",
    "    - Score of the matrix\n",
    "    \"\"\"\n",
    "    #flatter the matrix:\n",
    "    matrice = matrix.flatten()\n",
    "    score = 0\n",
    "    for i in range(len(matrice)):\n",
    "        score += (10**i)*matrice[i]\n",
    "    return int(score)\n",
    "\n",
    "\n",
    "def simplified_matrix_score(matrix):\n",
    "    \"\"\"\n",
    "    Performs the product for each element with the position in the matrix.\n",
    "    \n",
    "    Parameters:\n",
    "    - state (np.ndarray): A state of the puzzle.\n",
    "    \n",
    "    Returns:\n",
    "    - Score of the matrix\n",
    "    \"\"\"\n",
    "    #flatter the matrix:\n",
    "    matrice = matrix.flatten()\n",
    "    score = 0\n",
    "    for i in range(len(matrice)):\n",
    "        score += i*matrice[i]\n",
    "    return int(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import namedtuple, deque\n",
    "import numpy as np\n",
    "\n",
    "# Define action as a named tuple\n",
    "action = namedtuple('Action', ['pos1', 'pos2'])\n",
    "\n",
    "def depth_limited_search(initial_state: np.ndarray, final_state: np.ndarray, max_depth: int) -> list[action] or None:\n",
    "    \"\"\"\n",
    "    Performs a depth-limited search to solve the n-puzzle.\n",
    "    \n",
    "    Parameters:\n",
    "    - initial_state (np.ndarray): The starting state of the puzzle.\n",
    "    - final_state (np.ndarray): The desired goal state of the puzzle.\n",
    "    - max_depth (int): The maximum depth limit for the search.\n",
    "    \n",
    "    Returns:\n",
    "    - list[action] or None: A sequence of actions leading to the solution, or None if no solution is found.\n",
    "    \"\"\"\n",
    "    stack = deque([(initial_state, [], 0)])  # Stack of (current state, path to reach it, current depth)\n",
    "    visited = set()  # Set of visited states for current path only\n",
    "    optimum = matrix_score(final_state)\n",
    "\n",
    "    while stack:\n",
    "        current_state, path, depth = stack.pop()\n",
    "        current_score = matrix_score(current_state)\n",
    "\n",
    "        # Check if we reached the goal\n",
    "        if current_score == optimum:\n",
    "            return path\n",
    "        \n",
    "        # # Backtrack if depth limit reached\n",
    "        # if depth >= max_depth:\n",
    "        #     continue\n",
    "        \n",
    "        # Add current state to visited set (track only in current path)\n",
    "        visited.add(current_score)\n",
    "        \n",
    "        # Generate and iterate over all possible moves\n",
    "        for act in available_actions(current_state):\n",
    "            next_state = do_action(current_state, act)\n",
    "            \n",
    "            # Check if the next state has already been visited in the current path\n",
    "            if matrix_score(next_state) not in visited:\n",
    "                # Add the new state and path to stack, increase depth\n",
    "                stack.append((next_state, path + [act], depth + 1))\n",
    "        \n",
    "        # Remove the current state from visited set after backtracking\n",
    "        #visited.remove(matrix_score(current_state))\n",
    "    \n",
    "    return None  # Return None if no solution is found within depth limit\n",
    "\n",
    "# Iterative Deepening Depth-First Search (IDDFS) wrapper function\n",
    "def iterative_deepening_dfs(initial_state: np.ndarray, final_state: np.ndarray, max_depth: int = 50) -> list[action] or None:\n",
    "    \"\"\"\n",
    "    Performs an iterative deepening DFS to solve the n-puzzle.\n",
    "    \n",
    "    Parameters:\n",
    "    - initial_state (np.ndarray): The starting state of the puzzle.\n",
    "    - final_state (np.ndarray): The desired goal state of the puzzle.\n",
    "    - max_depth (int): The maximum depth to limit the search for IDDFS.\n",
    "    \n",
    "    Returns:\n",
    "    - list[action] or None: A sequence of actions leading to the solution, or None if no solution is found.\n",
    "    \"\"\"\n",
    "    for depth in range(1, max_depth + 1):\n",
    "        result = depth_limited_search(initial_state, final_state, depth)\n",
    "        if result is not None:\n",
    "            return result\n",
    "    return None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the context of the n-puzzle problem, **Manhattan distance** is a heuristic used to estimate how close a puzzle state is to the goal. For two matrices (representing the current state and the goal state), Manhattan distance is calculated by summing up the individual distances of each tile from its target position.\n",
    "\n",
    "1. **Tile Distance**: For each tile, the distance is the absolute difference between its current row and target row, plus the absolute difference between its current column and target column.\n",
    "   \n",
    "2. **Total Distance**: Summing these values for all tiles gives the Manhattan distance for the puzzle. This value represents the minimum number of moves required to reach the goal state if we could move each tile independently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def manhattan_distance(state: np.ndarray, goal_state: np.ndarray) -> int:\n",
    "    \"\"\"\n",
    "    Calculates the Manhattan distance for each tile in the puzzle.\n",
    "    \"\"\"\n",
    "    distance = 0\n",
    "    for value in range(1, state.size):  # Skip 0 (empty space)\n",
    "        target_pos = np.argwhere(goal_state == value)[0]\n",
    "        current_pos = np.argwhere(state == value)[0]\n",
    "        distance += abs(target_pos[0] - current_pos[0]) + abs(target_pos[1] - current_pos[1])\n",
    "    return distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define action as a named tuple\n",
    "action = namedtuple('Action', ['pos1', 'pos2'])\n",
    "\n",
    "def a_star_search(initial_state: np.ndarray, final_state: np.ndarray) -> list[action] or None:\n",
    "    \"\"\"\n",
    "    Performs an A* search to solve the n-puzzle.\n",
    "    \n",
    "    Parameters:\n",
    "    - initial_state (np.ndarray): The starting state of the puzzle.\n",
    "    - final_state (np.ndarray): The desired goal state of the puzzle.\n",
    "    \n",
    "    Returns:\n",
    "    - list[action] or None: A sequence of actions leading to the solution, or None if no solution is found.\n",
    "    \"\"\"\n",
    "    # Priority queue: (f_score, g_score, current_state, path)\n",
    "    open_set = []\n",
    "    heappush(open_set, (manhattan_distance(initial_state, final_state), 0, initial_state.tobytes(), []))\n",
    "    #this function push into open_set a new element continuing to garantee the heap properties.\n",
    "    # the element that we insert are 4.\n",
    "    \n",
    "    visited = set()  \n",
    "    \n",
    "    while open_set:\n",
    "        f_score, g_score, current_bytes, path = heappop(open_set)\n",
    "        current_state = np.frombuffer(current_bytes, dtype=initial_state.dtype).reshape(initial_state.shape)\n",
    "        \n",
    "        # Check if we reached the goal\n",
    "        if np.array_equal(current_state, final_state):\n",
    "            return path\n",
    "        \n",
    "        # Mark current state as visited\n",
    "        visited.add(current_bytes)\n",
    "        \n",
    "        # Generate and iterate over all possible moves\n",
    "        for act in available_actions(current_state):\n",
    "            next_state = do_action(current_state, act)\n",
    "            next_bytes = next_state.tobytes()\n",
    "            if next_bytes in visited:\n",
    "                continue\n",
    "            \n",
    "            # Calculate scores for the next state\n",
    "            new_g_score = g_score + 1\n",
    "            new_f_score = new_g_score + manhattan_distance(next_state, final_state)\n",
    "            \n",
    "            # Push the new state into the priority queue with updated scores\n",
    "            heappush(open_set, (new_f_score, new_g_score, next_bytes, path + [act]))\n",
    "    \n",
    "    return None  # Return None if no solution is found"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Updated Depth search\n",
    "\n",
    "At each iteration, among all the possible we choose the one with the minimum manahattan distance to the test goal.\n",
    "In case there is no state with less manhattan distance compared to the current we select the oen that is the \"best\" among the possibilities. In case they are the same we use the last one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_with_A(initial_state: np.ndarray, final_state: np.ndarray) -> list or None:\n",
    "    stack = deque([(initial_state, [], 0)])  # Stack of (current state, path to reach it, current depth)\n",
    "    visited = set()  # Set of visited states for current path only\n",
    "    optimum = matrix_score(final_state)\n",
    "\n",
    "    while stack:\n",
    "        current_state, path, depth = stack.pop()\n",
    "        current_score = matrix_score(current_state)\n",
    "\n",
    "        # Check if we reached the goal\n",
    "        if current_score == optimum:\n",
    "            return path\n",
    "        \n",
    "        # Add current state to visited set (track only in current path)\n",
    "        visited.add(current_score)\n",
    "        \n",
    "        # Generate and calculate Manhattan distances for all possible moves\n",
    "        successors = []\n",
    "        for act in available_actions(current_state):\n",
    "            next_state = do_action(current_state, act)\n",
    "            next_score = matrix_score(next_state)\n",
    "            \n",
    "            # Check if the next state has already been visited in the current path\n",
    "            if next_score not in visited:\n",
    "                # Calculate the Manhattan distance from the goal\n",
    "                manhattan_dist = manhattan_distance(next_state, final_state)\n",
    "                # Append (next_state, path + [act], depth + 1, manhattan_dist) to successors list\n",
    "                successors.append((next_state, path + [act], depth + 1, manhattan_dist))\n",
    "        \n",
    "        # Sort successors by Manhattan distance (ascending)\n",
    "        successors.sort(key=lambda x: x[3])  # Sort by the fourth item, which is manhattan_dist\n",
    "        \n",
    "        # Add sorted successors to the stack\n",
    "        for next_state, new_path, new_depth, _ in successors:\n",
    "            stack.append((next_state, new_path, new_depth))\n",
    "        \n",
    "        # Remove the current state from visited set after backtracking\n",
    "        #visited.remove(current_score)  # Uncomment if backtracking is implemented\n",
    "    \n",
    "    return None  # Return None if no solution is found within depth limit\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Breath first ti fa trovare una soluzione ottima. Depth la prima soluzione ottima."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m test_goal \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mappend(test_goal, \u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m      3\u001b[0m test_goal \u001b[38;5;241m=\u001b[39m test_goal\u001b[38;5;241m.\u001b[39mreshape((PUZZLE_DIM, PUZZLE_DIM))\n\u001b[1;32m----> 4\u001b[0m \u001b[43miterative_deepening_dfs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_goal\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[7], line 66\u001b[0m, in \u001b[0;36miterative_deepening_dfs\u001b[1;34m(initial_state, final_state, max_depth)\u001b[0m\n\u001b[0;32m     54\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     55\u001b[0m \u001b[38;5;124;03mPerforms an iterative deepening DFS to solve the n-puzzle.\u001b[39;00m\n\u001b[0;32m     56\u001b[0m \u001b[38;5;124;03m\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     63\u001b[0m \u001b[38;5;124;03m- list[action] or None: A sequence of actions leading to the solution, or None if no solution is found.\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     65\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m depth \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, max_depth \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m---> 66\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mdepth_limited_search\u001b[49m\u001b[43m(\u001b[49m\u001b[43minitial_state\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfinal_state\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdepth\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     67\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m     68\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "Cell \u001b[1;32mIn[7], line 39\u001b[0m, in \u001b[0;36mdepth_limited_search\u001b[1;34m(initial_state, final_state, max_depth)\u001b[0m\n\u001b[0;32m     36\u001b[0m visited\u001b[38;5;241m.\u001b[39madd(current_score)\n\u001b[0;32m     38\u001b[0m \u001b[38;5;66;03m# Generate and iterate over all possible moves\u001b[39;00m\n\u001b[1;32m---> 39\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m act \u001b[38;5;129;01min\u001b[39;00m \u001b[43mavailable_actions\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcurrent_state\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[0;32m     40\u001b[0m     next_state \u001b[38;5;241m=\u001b[39m do_action(current_state, act)\n\u001b[0;32m     42\u001b[0m     \u001b[38;5;66;03m# Check if the next state has already been visited in the current path\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[3], line 7\u001b[0m, in \u001b[0;36mavailable_actions\u001b[1;34m(state)\u001b[0m\n\u001b[0;32m      5\u001b[0m     actions\u001b[38;5;241m.\u001b[39mappend(action((x, y), (x \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m, y)))\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m x \u001b[38;5;241m<\u001b[39m PUZZLE_DIM \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m----> 7\u001b[0m     actions\u001b[38;5;241m.\u001b[39mappend(\u001b[43maction\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m y \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m      9\u001b[0m     actions\u001b[38;5;241m.\u001b[39mappend(action((x, y), (x, y \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m)))\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "test_goal = np.arange(1, PUZZLE_DIM*PUZZLE_DIM, 1)\n",
    "test_goal = np.append(test_goal, 0)\n",
    "test_goal = test_goal.reshape((PUZZLE_DIM, PUZZLE_DIM))\n",
    "iterative_deepening_dfs(state, test_goal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Action(pos1=(1, 1), pos2=(0, 1)),\n",
       " Action(pos1=(0, 1), pos2=(0, 0)),\n",
       " Action(pos1=(0, 0), pos2=(1, 0)),\n",
       " Action(pos1=(1, 0), pos2=(1, 1)),\n",
       " Action(pos1=(1, 1), pos2=(1, 2)),\n",
       " Action(pos1=(1, 2), pos2=(2, 2)),\n",
       " Action(pos1=(2, 2), pos2=(2, 1)),\n",
       " Action(pos1=(2, 1), pos2=(1, 1)),\n",
       " Action(pos1=(1, 1), pos2=(0, 1)),\n",
       " Action(pos1=(0, 1), pos2=(0, 2)),\n",
       " Action(pos1=(0, 2), pos2=(1, 2)),\n",
       " Action(pos1=(1, 2), pos2=(2, 2)),\n",
       " Action(pos1=(2, 2), pos2=(2, 1)),\n",
       " Action(pos1=(2, 1), pos2=(2, 0)),\n",
       " Action(pos1=(2, 0), pos2=(1, 0)),\n",
       " Action(pos1=(1, 0), pos2=(1, 1)),\n",
       " Action(pos1=(1, 1), pos2=(1, 2)),\n",
       " Action(pos1=(1, 2), pos2=(0, 2)),\n",
       " Action(pos1=(0, 2), pos2=(0, 1)),\n",
       " Action(pos1=(0, 1), pos2=(1, 1)),\n",
       " Action(pos1=(1, 1), pos2=(2, 1)),\n",
       " Action(pos1=(2, 1), pos2=(2, 2))]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a_star_search(state, test_goal)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
