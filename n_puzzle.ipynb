{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deadline: 24 novembre ore 23:59.\n",
    "Solve efficiently a generic n^2-1 puzzle using path search algorithm.\n",
    "\n",
    "Cost=  total number of actions you need to __evaluate__. An action is something that bring me to a new state. For example the number of swaps to do.\n",
    "\n",
    "The result is the sequence of action that took you at the end. The goal is not to find a state but a sequence of actions from srtarting point to end point: we do not look for a soluzion but for a sequence of actions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import and Inizialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import namedtuple\n",
    "from random import choice\n",
    "from tqdm.auto import tqdm\n",
    "import numpy as np\n",
    "from heapq import heappop, heappush"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PUZZLE_DIM = 4\n",
    "action = namedtuple('Action', ['pos1', 'pos2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def available_actions(state: np.ndarray) -> list['Action']:\n",
    "    x, y = [int(_[0]) for _ in np.where(state == 0)]\n",
    "    actions = list()\n",
    "    if x > 0:\n",
    "        actions.append(action((x, y), (x - 1, y)))\n",
    "    if x < PUZZLE_DIM - 1:\n",
    "        actions.append(action((x, y), (x + 1, y)))\n",
    "    if y > 0:\n",
    "        actions.append(action((x, y), (x, y - 1)))\n",
    "    if y < PUZZLE_DIM - 1:\n",
    "        actions.append(action((x, y), (x, y + 1)))\n",
    "    return actions\n",
    "\n",
    "\n",
    "\n",
    "def do_action(state: np.ndarray, action: 'Action') -> np.ndarray:\n",
    "    new_state = state.copy()\n",
    "    new_state[action.pos1], new_state[action.pos2] = new_state[action.pos2], new_state[action.pos1]\n",
    "    return new_state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The state is a numpy array.\n",
    "\n",
    "We created a function that returns the number of actions from a state pos1 to a state pos2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute 100_000 random actions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Randomizing: 100%|██████████| 100000/100000 [00:01<00:00, 66562.18it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 7,  8, 15, 13],\n",
       "       [ 1,  2, 10,  0],\n",
       "       [ 5, 11, 14,  4],\n",
       "       [ 9,  3, 12,  6]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "RANDOMIZE_STEPS = 100_000\n",
    "state = np.array([i for i in range(1, PUZZLE_DIM**2)] + [0]).reshape((PUZZLE_DIM, PUZZLE_DIM))\n",
    "for r in tqdm(range(RANDOMIZE_STEPS), desc='Randomizing'):\n",
    "    state = do_action(state, choice(available_actions(state)))\n",
    "state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a function that indicates if we end the search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_goal(solution):\n",
    "    arr_solution = np.reshape(solution, PUZZLE_DIM*PUZZLE_DIM)\n",
    "    arr_solution_no_zero = arr_solution[0: len(arr_solution)-1]\n",
    "    if np.all(arr_solution_no_zero[:-1] <= arr_solution_no_zero[1:]) and arr_solution[len(arr_solution)-1]==0:\n",
    "        return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Depth search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use a function that computes a value for each matrix so that it will be easy to store matrices.\n",
    "\n",
    "This function shouuld be bi-directional: a number is associated to only one matrix and viceversa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def matrix_score(matrix):\n",
    "    \"\"\"\n",
    "    Performs the product for each element with the position in the matrix.\n",
    "    \n",
    "    Parameters:\n",
    "    - state (np.ndarray): A state of the puzzle.\n",
    "    \n",
    "    Returns:\n",
    "    - Score of the matrix\n",
    "    \"\"\"\n",
    "    #flatter the matrix:\n",
    "    matrice = matrix.flatten()\n",
    "    score = 0\n",
    "    for i in range(len(matrice)):\n",
    "        score += (10**i)*matrice[i]\n",
    "    return int(score)\n",
    "\n",
    "\n",
    "def simplified_matrix_score(matrix):\n",
    "    \"\"\"\n",
    "    Performs the product for each element with the position in the matrix.\n",
    "    \n",
    "    Parameters:\n",
    "    - state (np.ndarray): A state of the puzzle.\n",
    "    \n",
    "    Returns:\n",
    "    - Score of the matrix\n",
    "    \"\"\"\n",
    "    #flatter the matrix:\n",
    "    matrice = matrix.flatten()\n",
    "    score = 0\n",
    "    for i in range(len(matrice)):\n",
    "        score += i*matrice[i]\n",
    "    return int(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import namedtuple, deque\n",
    "import numpy as np\n",
    "\n",
    "# Define action as a named tuple\n",
    "action = namedtuple('Action', ['pos1', 'pos2'])\n",
    "\n",
    "def depth_limited_search(initial_state: np.ndarray, final_state: np.ndarray, max_depth: int) -> list[action] or None:\n",
    "    \"\"\"\n",
    "    Performs a depth-limited search to solve the n-puzzle.\n",
    "    \n",
    "    Parameters:\n",
    "    - initial_state (np.ndarray): The starting state of the puzzle.\n",
    "    - final_state (np.ndarray): The desired goal state of the puzzle.\n",
    "    - max_depth (int): The maximum depth limit for the search.\n",
    "    \n",
    "    Returns:\n",
    "    - list[action] or None: A sequence of actions leading to the solution, or None if no solution is found.\n",
    "    \"\"\"\n",
    "    stack = deque([(initial_state, [], 0)])  # Stack of (current state, path to reach it, current depth)\n",
    "    visited = set()  # Set of visited states for current path only\n",
    "    optimum = matrix_score(final_state)\n",
    "\n",
    "    while stack:\n",
    "        current_state, path, depth = stack.pop()\n",
    "        current_score = matrix_score(current_state)\n",
    "\n",
    "        # Check if we reached the goal\n",
    "        if current_score == optimum:\n",
    "            return path\n",
    "        \n",
    "        # # Backtrack if depth limit reached\n",
    "        # if depth >= max_depth:\n",
    "        #     continue\n",
    "        \n",
    "        # Add current state to visited set (track only in current path)\n",
    "        visited.add(current_score)\n",
    "        \n",
    "        # Generate and iterate over all possible moves\n",
    "        for act in available_actions(current_state):\n",
    "            next_state = do_action(current_state, act)\n",
    "            \n",
    "            # Check if the next state has already been visited in the current path\n",
    "            if matrix_score(next_state) not in visited:\n",
    "                # Add the new state and path to stack, increase depth\n",
    "                stack.append((next_state, path + [act], depth + 1))\n",
    "        \n",
    "        # Remove the current state from visited set after backtracking\n",
    "        #visited.remove(matrix_score(current_state))\n",
    "    \n",
    "    return None  # Return None if no solution is found within depth limit\n",
    "\n",
    "# Iterative Deepening Depth-First Search (IDDFS) wrapper function\n",
    "def iterative_deepening_dfs(initial_state: np.ndarray, final_state: np.ndarray, max_depth: int = 50) -> list[action] or None:\n",
    "    \"\"\"\n",
    "    Performs an iterative deepening DFS to solve the n-puzzle.\n",
    "    \n",
    "    Parameters:\n",
    "    - initial_state (np.ndarray): The starting state of the puzzle.\n",
    "    - final_state (np.ndarray): The desired goal state of the puzzle.\n",
    "    - max_depth (int): The maximum depth to limit the search for IDDFS.\n",
    "    \n",
    "    Returns:\n",
    "    - list[action] or None: A sequence of actions leading to the solution, or None if no solution is found.\n",
    "    \"\"\"\n",
    "    for depth in range(1, max_depth + 1):\n",
    "        result = depth_limited_search(initial_state, final_state, depth)\n",
    "        if result is not None:\n",
    "            return result\n",
    "    return None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Updated Depth search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At each iteration, among all the possible we choose the one with the minimum manahattan distance to the test goal.\n",
    "In case there is no state with less manhattan distance compared to the current we select the oen that is the \"best\" among the possibilities. In case they are the same we use the last one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_depth(initial_state: np.ndarray, final_state: np.ndarray) -> list or None:\n",
    "    stack = deque([(initial_state, [], 0)])  # Stack of (current state, path to reach it, current depth)\n",
    "    visited = set()  # Set of visited states for current path only\n",
    "    optimum = matrix_score(final_state)\n",
    "\n",
    "    while stack:\n",
    "        current_state, path, depth = stack.pop()\n",
    "        current_score = matrix_score(current_state)\n",
    "\n",
    "        # Check if we reached the goal\n",
    "        if current_score == optimum:\n",
    "            return path\n",
    "        \n",
    "        # Add current state to visited set (track only in current path)\n",
    "        visited.add(current_score)\n",
    "        \n",
    "        # Generate and calculate Manhattan distances for all possible moves\n",
    "        successors = []\n",
    "        for act in available_actions(current_state):\n",
    "            next_state = do_action(current_state, act)\n",
    "            next_score = matrix_score(next_state)\n",
    "            \n",
    "            # Check if the next state has already been visited in the current path\n",
    "            if next_score not in visited:\n",
    "                # Calculate the Manhattan distance from the goal\n",
    "                manhattan_dist = manhattan_distance(next_state, final_state)\n",
    "                # Append (next_state, path + [act], depth + 1, manhattan_dist) to successors list\n",
    "                successors.append((next_state, path + [act], depth + 1, manhattan_dist))\n",
    "        \n",
    "        # Sort successors by Manhattan distance (ascending)\n",
    "        successors.sort(key=lambda x: x[3])  # Sort by the fourth item, which is manhattan_dist\n",
    "        \n",
    "        # Add sorted successors to the stack\n",
    "        for next_state, new_path, new_depth, _ in successors:\n",
    "            stack.append((next_state, new_path, new_depth))\n",
    "        \n",
    "        # Remove the current state from visited set after backtracking\n",
    "        #visited.remove(current_score)  # Uncomment if backtracking is implemented\n",
    "    \n",
    "    return None  # Return None if no solution is found within depth limit\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the context of the n-puzzle problem, **Manhattan distance** is a heuristic used to estimate how close a puzzle state is to the goal. For two matrices (representing the current state and the goal state), Manhattan distance is calculated by summing up the individual distances of each tile from its target position.\n",
    "\n",
    "1. **Tile Distance**: For each tile, the distance is the absolute difference between its current row and target row, plus the absolute difference between its current column and target column.\n",
    "   \n",
    "2. **Total Distance**: Summing these values for all tiles gives the Manhattan distance for the puzzle. This value represents the minimum number of moves required to reach the goal state if we could move each tile independently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def manhattan_distance(state: np.ndarray, goal_state: np.ndarray) -> int:\n",
    "    \"\"\"\n",
    "    Calculates the Manhattan distance for each tile in the puzzle.\n",
    "    \"\"\"\n",
    "    distance = 0\n",
    "    for value in range(1, state.size):  # Skip 0 (empty space)\n",
    "        target_pos = np.argwhere(goal_state == value)[0]\n",
    "        current_pos = np.argwhere(state == value)[0]\n",
    "        distance += abs(target_pos[0] - current_pos[0]) + abs(target_pos[1] - current_pos[1])\n",
    "    return distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def a_star(initial_state: np.ndarray, final_state: np.ndarray) -> list[action] or None:\n",
    "    \"\"\"\n",
    "    Performs an A* search to solve the n-puzzle.\n",
    "    \n",
    "    Parameters:\n",
    "    - initial_state (np.ndarray): The starting state of the puzzle.\n",
    "    - final_state (np.ndarray): The desired goal state of the puzzle.\n",
    "    \n",
    "    Returns:\n",
    "    - list[action] or None: A sequence of actions leading to the solution, or None if no solution is found.\n",
    "    \"\"\"\n",
    "    # Priority queue: (f_score, g_score, current_state, path)\n",
    "    open_set = []\n",
    "    heappush(open_set, (manhattan_distance(initial_state, final_state), 0, initial_state.tobytes(), []))\n",
    "    #this function push into open_set a new element continuing to garantee the heap properties.\n",
    "    # the element that we insert are 4.\n",
    "    \n",
    "    visited = set()  \n",
    "    \n",
    "    while open_set:\n",
    "        f_score, g_score, current_bytes, path = heappop(open_set)\n",
    "        current_state = np.frombuffer(current_bytes, dtype=initial_state.dtype).reshape(initial_state.shape)\n",
    "        \n",
    "        # Check if we reached the goal\n",
    "        if np.array_equal(current_state, final_state):\n",
    "            return path\n",
    "        \n",
    "        # Mark current state as visited\n",
    "        visited.add(current_bytes)\n",
    "        \n",
    "        # Generate and iterate over all possible moves\n",
    "        for act in available_actions(current_state):\n",
    "            next_state = do_action(current_state, act)\n",
    "            next_bytes = next_state.tobytes()\n",
    "            if next_bytes in visited:\n",
    "                continue\n",
    "            \n",
    "            # Calculate scores for the next state\n",
    "            new_g_score = g_score + 1\n",
    "            new_f_score = new_g_score + manhattan_distance(next_state, final_state)\n",
    "            \n",
    "            # Push the new state into the priority queue with updated scores\n",
    "            heappush(open_set, (new_f_score, new_g_score, next_bytes, path + [act]))\n",
    "    \n",
    "    return None  # Return None if no solution is found"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this code we are storing all the state and comparing them. This may be difficoult. Let's try to use the function that computes a singol number for a given state. This doesn't provide much improvement (find the code in test file)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The problem is that still we are not able to solve 4*4 puzzle."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At first we will try to use a function to be more efficient for storing and comparing the state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def state_to_tuple(state: np.ndarray) -> tuple:\n",
    "    return tuple(state.flatten())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use this function and also change the way we implemented the h function of A*: instead of using only Manhattan distance, we also include other functions, such as linear conflict and we combine the two. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We may also consider another approach that returns a number representative for the state. This is possible using the hash function over the state:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def state_to_hash(state: np.ndarray) -> int:\n",
    "    return int(\"\".join(map(str, state.flatten())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We may also want to try to convert the numpy ndarray into a bytes object directly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def state_to_bytes(state: np.ndarray) -> bytes:\n",
    "    return state.tobytes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PuzzleHeuristicService:\n",
    "    def __init__(self, goal_state: np.ndarray):\n",
    "        self.goal_state = goal_state\n",
    "\n",
    "    def manhattan_distance(self, state: np.ndarray) -> int:\n",
    "        size = state.shape[0]\n",
    "        dist = 0\n",
    "        for i in range(size):\n",
    "            for j in range(size):\n",
    "                value = state[i, j]\n",
    "                if value != 0:  # Ignore the blanch space\n",
    "                    target_x, target_y = divmod(np.where(self.goal_state.flatten() == value)[0][0], size)\n",
    "                    dist += abs(i - target_x) + abs(j - target_y)\n",
    "        return dist\n",
    "\n",
    "    def linear_conflict(self, state: np.ndarray) -> int:\n",
    "        conflict = 0\n",
    "        size = state.shape[0]\n",
    "        for row in range(size):\n",
    "            row_goal = self.goal_state[row]\n",
    "            row_state = state[row]\n",
    "            for i in range(size):\n",
    "                for j in range(i + 1, size):\n",
    "                    if (\n",
    "                        row_state[i] in row_goal\n",
    "                        and row_state[j] in row_goal\n",
    "                        and row_state[i] > row_state[j]\n",
    "                    ):\n",
    "                        conflict += 2\n",
    "        return conflict\n",
    "\n",
    "    def combined_heuristic(self, state: np.ndarray) -> int:\n",
    "        #return self.manhattan_distance(state) + self.linear_conflict(state)\n",
    "        return self.manhattan_distance(state)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def enhanced_a_star(initial_state: np.ndarray, final_state: np.ndarray) -> list or None:\n",
    "    \"\"\"\n",
    "    Enhanced A* algorithm for the n-puzzle problem using modular heuristics.\n",
    "    \"\"\"\n",
    "    heuristic_service = PuzzleHeuristicService(final_state)\n",
    "\n",
    "    def calculate_heuristic(state: np.ndarray) -> int:\n",
    "        return heuristic_service.combined_heuristic(state)\n",
    "\n",
    "    # Priority queue: (f_score, g_score, current_state, path)\n",
    "    open_set = []\n",
    "    heappush(open_set, (calculate_heuristic(initial_state), 0, initial_state.tobytes(), []))\n",
    "    visited = set()\n",
    "    optimum = state_to_bytes(final_state)\n",
    "\n",
    "    while open_set:\n",
    "        # Extract node with the lowest f score (f score= cost)\n",
    "        f_score, g_score, current_bytes, path = heappop(open_set)\n",
    "        current_state = np.frombuffer(current_bytes, dtype=initial_state.dtype).reshape(initial_state.shape)\n",
    "        current_score = state_to_bytes(current_state)\n",
    "\n",
    "        # Check if we finished already:\n",
    "        if current_score == optimum:\n",
    "            return path  \n",
    "\n",
    "        # Add current node to visited\n",
    "        visited.add(current_score)\n",
    "\n",
    "        # Generate possible moves:\n",
    "        for act in available_actions(current_state):\n",
    "            next_state = do_action(current_state, act)\n",
    "            next_score = state_to_bytes(next_state)\n",
    "            if next_score in visited:\n",
    "                continue\n",
    "\n",
    "            # update scores:\n",
    "            new_g_score = g_score + 1\n",
    "            new_f_score = new_g_score + calculate_heuristic(next_state)\n",
    "\n",
    "            # Add new state to openset\n",
    "            heappush(open_set, (new_f_score, new_g_score, next_state.tobytes(), path + [act]))\n",
    "\n",
    "    return None  # No solution found\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After some experiments we can say that using only the manhattan distance improves the performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_goal = np.arange(1, PUZZLE_DIM*PUZZLE_DIM, 1)\n",
    "test_goal = np.append(test_goal, 0)\n",
    "test_goal = test_goal.reshape((PUZZLE_DIM, PUZZLE_DIM))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Try the upgrade depth search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#update_depth(state, test_goal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Try the A* algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#a_star(state, test_goal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Action(pos1=(1, 3), pos2=(2, 3)),\n",
       " Action(pos1=(2, 3), pos2=(3, 3)),\n",
       " Action(pos1=(3, 3), pos2=(3, 2)),\n",
       " Action(pos1=(3, 2), pos2=(2, 2)),\n",
       " Action(pos1=(2, 2), pos2=(1, 2)),\n",
       " Action(pos1=(1, 2), pos2=(0, 2)),\n",
       " Action(pos1=(0, 2), pos2=(0, 1)),\n",
       " Action(pos1=(0, 1), pos2=(1, 1)),\n",
       " Action(pos1=(1, 1), pos2=(2, 1)),\n",
       " Action(pos1=(2, 1), pos2=(2, 2)),\n",
       " Action(pos1=(2, 2), pos2=(1, 2)),\n",
       " Action(pos1=(1, 2), pos2=(0, 2)),\n",
       " Action(pos1=(0, 2), pos2=(0, 3)),\n",
       " Action(pos1=(0, 3), pos2=(1, 3)),\n",
       " Action(pos1=(1, 3), pos2=(1, 2)),\n",
       " Action(pos1=(1, 2), pos2=(0, 2)),\n",
       " Action(pos1=(0, 2), pos2=(0, 1)),\n",
       " Action(pos1=(0, 1), pos2=(0, 0)),\n",
       " Action(pos1=(0, 0), pos2=(1, 0)),\n",
       " Action(pos1=(1, 0), pos2=(2, 0)),\n",
       " Action(pos1=(2, 0), pos2=(2, 1)),\n",
       " Action(pos1=(2, 1), pos2=(3, 1)),\n",
       " Action(pos1=(3, 1), pos2=(3, 2)),\n",
       " Action(pos1=(3, 2), pos2=(2, 2)),\n",
       " Action(pos1=(2, 2), pos2=(1, 2)),\n",
       " Action(pos1=(1, 2), pos2=(1, 1)),\n",
       " Action(pos1=(1, 1), pos2=(2, 1)),\n",
       " Action(pos1=(2, 1), pos2=(2, 2)),\n",
       " Action(pos1=(2, 2), pos2=(2, 3)),\n",
       " Action(pos1=(2, 3), pos2=(3, 3)),\n",
       " Action(pos1=(3, 3), pos2=(3, 2)),\n",
       " Action(pos1=(3, 2), pos2=(3, 1)),\n",
       " Action(pos1=(3, 1), pos2=(2, 1)),\n",
       " Action(pos1=(2, 1), pos2=(2, 2)),\n",
       " Action(pos1=(2, 2), pos2=(1, 2)),\n",
       " Action(pos1=(1, 2), pos2=(1, 1)),\n",
       " Action(pos1=(1, 1), pos2=(0, 1)),\n",
       " Action(pos1=(0, 1), pos2=(0, 2)),\n",
       " Action(pos1=(0, 2), pos2=(1, 2)),\n",
       " Action(pos1=(1, 2), pos2=(1, 1)),\n",
       " Action(pos1=(1, 1), pos2=(2, 1)),\n",
       " Action(pos1=(2, 1), pos2=(2, 0)),\n",
       " Action(pos1=(2, 0), pos2=(3, 0)),\n",
       " Action(pos1=(3, 0), pos2=(3, 1)),\n",
       " Action(pos1=(3, 1), pos2=(3, 2)),\n",
       " Action(pos1=(3, 2), pos2=(3, 3))]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "enhanced_a_star(state, test_goal)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
